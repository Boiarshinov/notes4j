---
title: "Apache Cassandra"
tags:
  - database
  - nosql
  - java_and_db
  - distributed_system
draft: false
---

# Apache Cassandra

Apache Cassandra - это распределенная NoSQL база данных.

Основные свойства Apache Cassandra:
- Распределенность - можно разворачивать кластер нод, которые шардируют данные между собой.
- Децентрализованность - нет мастера и соответственно единой точки отказа для любой из операций.
- Отказоустойчивость - за счет репликации.
- Линейная масштабируемость - добавление новых нод и / или новых данных не замедляет работу системы в целом.

Apache Cassandra для запросов использует специальный язык [CQL](cql.md), похожий на SQL.

Традиционно Cassandra считается AP системой в терминах CAP-теоремы.
Хотя благодаря возможности задания уровня консистентности (см. ниже) для каждого запроса, в некоторых случаях она может становиться CP-системой.

Apache Cassandra начинала разрабатываться в Facebook, вдохновляемая другими нереляционными БД: Amazon Dynamo и Google Big Table.
Сейчас Apache Cassandra развивается под крылом Apache Software Foundation, но ее основным мейнтейнером является компания DataStax.

Из известных компаний используют Apache Cassandra:
- Instagram
- Netflix (10'000 нод в 100 кластерах)
- Apple (150'000 нод)
- GitHub


## Распределенность

Apache Cassandra развертывывается в виде кластера нод. 
При этом нет единой точки отказа, все ноды являются равноценными.
Такие системы также называют masterless.
Ноды Cassandra объединяются в кольца, когда каждая нода взаимодействует только с двумя соседними.

Ноды всегда знают о состоянии кластера.
Ноды передают информацию о состоянии друг друга с помощью gossip protocol.
Для синхронизации данных между нодами в Cassandra используется PAXOS.

Для того чтобы данные не пропадали при выходе из строя одной из нод, есть некоторая избыточность в хранении данных.
На скольки серверах будут отреплицированны данные определяется настройкой replication factor.
Рекомендуемое значение replication factor - 3 или 5.
Четные значения не рекомендуются, т.к. при них кворум будет избыточным.
Единичный кусок данных, который будет реплицироваться на другую ноду, называется партицией. 

Когда запрос на запись приходит на любую из нод, она определяет в каких нодах располагается запрашиваемая партиция и перекидывает данные туда.
Т.е. все ноды имеют информацию о том, как распределены партиции по остальным нодам.
Нода, принявшая запрос, называется нодой-**координатором**.
Если одна из нод держателей партиции недоступна, то нода-координатор записывает себе, что нужно будет отреплицировать данные в нее, когда она поднимется.

При добавлении новой ноды в кластер или выводе ноды из кластера данные перераспределяются между нодами.
Происходит перебалансировка кластера.
Рекомендуется ноды вводить/выводить постепенно, чтобы не создавать излишнюю нагрузку на сеть.

Cassandra позволяет синхронизировать данные между кластерами, которые находятся в разных датацентрах (ЦОДах).
Для этого нода-координатор, принявшая запрос, реплицирует его на любую ноду другого кластера.

### Consistency level
В каждом запросе к Cassandra есть специальный элемент - уровень консистентности (`consistency level`).
Уровень консистентности определяет количество нод, от которых нужно дождаться ответа, чтобы считать запрос выполненным.
Возможные значения уровня консистентности:
- `ANY` - если запрос достиг координатор-ноды, то он уже считается обработанным. Работает только на запись. Не рекомендуется к использованию
- `ONE`, `TWO`, `THREE` - достаточно ответа от одной / двух / трех нод
- `QUORUM` - достаточно ответа от большинства нод, обслуживающих партицию. Для `replication factor = 3`, кворумом будет считаться 2 ноды. Для двух ЦОД с тем же фактором репликации, кворумом будет считаться 4 ноды.
- `LOCAL_QUORUM` - достаточно ответа от большинства нод, обслуживающих партицию на данном ЦОД. Для `replication factor = 3` и любом количество ЦОДов, кворумом будет считаться 2 ноды
- `EACH_QUORUM` - достаточно ответа от большинства нод, обслуживающих партицию на всех ЦОДах при записи. При чтении достаточно локального кворума
- `ALL` - нужен ответ от всех нод, обслуживающих партицию

По умолчанию Cassandra использует `QUORUM` и для операций на чтение, и для операций на запись.

### Hinted Handoff
Если координатор не получил акновледжа от одной из нод, но при этом уже отдал акновледж клиенту, то он запишет этот факт в специальный файлик hinted handoff и потом догонит данные до ноды.

По умолчанию hinted handoff включен и хранит данные в течение 3 часов.
По прошествии истечения времени, считаем, что нода сдохла и хранить для нее хинты уже не нужно.


### Драйвер
Драйвер - это библиотека, которая подключается к приложению и позволяет взаимодействовать с Apache Cassandra.

Драйвер при установлении соединения с кластером выгружает информацию о том, на каких нодах располагаются какие партиции данных.
Благодаря этому запросы будут идти на те ноды, на которых лежат нужные данные.
Драйвер вычисляет хэш из ключа партиции и смотрит, между какими токенами он лежит.
По токену определяет нода, владеющая записью.

Главный драйвер - джавовый. 
Драйверы для других языков - просто обертки над джавовым.

Стандартный джавовый драйвер **не** реализует стандарт [JDBC](../java/jdbc.md).
Существует проприетарный JDBC-драйвер от DataStax, который имеет урезанный функционал и в основном используется в утилитах миграции, специализирующихся на реляционных БД. 

<mark>Написать про java-driver в отдельной заметке</mark>

Политики балансировки:
- `RoundRobinPolicy` - раунд-робин по всем нодам кластера
- `DCAwareRoundRobinPolicy` - сначала раунд-робин по нодам своего ЦОД, потом по нодам других ЦОД
- `TokenAwarePolicy` - определяем ноду, владеющую данными по хэшу ключа партиционирования

Политики реконнекта:
- `ConstantReconnectionPolicy`
- `ExponentionalReconnectionPolicy` - попытки реконнекта с возрастающими экспоненциально таймаутами.

Политики повторов:
- `DefaultRetryPolicy` - ретраи с тем же уровнем консистентности, что и первоначальный запрос
- `DowngradingConsistencyRetryPolicy` - уменьшает уровень консистентности, если с первого раза не получилось. __Deprecated__
- `FallthroughRetryPolicy` - никаких повторов, сразу бросает исключение
- `LoggingRetryPolicy` - декоратор, который будет логировать попытки

Лучшие практики:
- Использовать `QUORUM` или `LOCAL_QUORUM`
- Использовать prepared statement
- Использовать балансировку `TokenAwarePolicy`




## Принцип работы с памятью

Когда в Cassandra приходит запрос на запись, первым делом координатор-нода записывает его в файл, называемый commit log. 
Commit log - это append only файлик, в который данные записываются всегда в конец.
Затем данные попадают в memtable - это структура, которая хранится в оперативной памяти ноды.
Для каждой записи Cassandra проставляет таймстемп, когда запись попала в ноду.
С некоторой периодичностью данные из memtable записываются в специальные файлы - **SSTable**, а память освобождается под новые записи.

> SSTable - от Sorted Strings Table. Название пошло из гугловой BigTable

Также регулярно проводится объединение отдельных sstable, при котором из файлов удаляются устаревшие данные.
Этот процесс называется **компакцией**.

Удаление в Cassandra - это новая запись, которая идет со специальным маркером.
Т.е. событие удаления записывается в commit log, затем в memtable, потом попадает в sstable, и только при компакции данные будут действительно удалены.
Такая логика работы может приводить к неожиданному поведению - сразу после запроса на удаление свободное дисковое пространство уменьшается и освободится только спустя какое-то время.

Обновление - тоже новая запись.
При этом запись может первоначально попасть не в ту sstable, в которой находилось предыдущее значение обновляемой сущности.
При компакции sstable'ов актуальное значение сущности среди различных ее версий выбирается по значению таймстампа.

При запросе на чтение Cassandra вычитывает все sstable, которые относятся к запрашиваемой партиции, ищет необходимую запись и при необходимости мержит результат.

### Commit log
Коммит лог может работать в двух режимах:
- `periodic` - отдаем клиенту акновледж сразу после получения данных в память. Данные из памяти в commit log сгружаются каждые 10 секунд.
- `batch` - данные сбрасываются на диск каждые 2 мс. Клиенту выдается подтверждение

Рекомендуется всегда выставлять режим `batch`. Но по умолчанию почему-то стоит `periodic`.
Т.к. если нода упадет, то будут потеряны данные, не сгруженные на диск почти за 10 секунд.


### DHT
DHT - distributed hash table

<mark>todo картиночка с кругом</mark>

vnode

### Компакция
Компакция - это слияние версий данных с одинаковым первичным ключом из нескольких sstable в одну.
Компакция бывает двух видов:
- minor - не работает с томбстоунами
- major - работает с томбстоунами

Томбстоуны не вычищаются при минорной компакции, потому что если полностью удалить данные с одной ноды, но при этом на другой ноде будет старая версия без томбстоуна, то везде будет восстановлена устаревшая версия.
Такие устаревшие версии называются **зомби**.
Поэтому перед удалением томбстоунов нужно сначала синхронизировать данные между нодами.

Рекомендуется перед вызовом мажорной компакции проводить node repair.

Стратегии компакции:
- `Size-Tiered Compaction Strategy` (STCS) - Используется по умолчанию
- `Leveled Compaction Strategy` (LCS) - данные сортируются между sstable. Файлы организуются в уровни. Но на каждом уровне первичный ключ может встретиться только один раз. Такой подход похож на B-дерево.
- `TimeWindow Compaction Strategy` (TWCS) - подходит для случаев, когда данные по ключу активно обновляются в течение какого-то промежутка времени, но затем фиксируются и больше никогда не обновляется. Т.е. для такой стратегии подходят таблицы, в которых частью первичного ключа является час / дата / неделя / месяц. TWCS пришел на смену DateTiered Compaction Strategy, который позволял устанавливать ширину окна только одним днем.


## Модель данных

В Cassandra на верхнем уровне хранения данных стоит **Keyspace** - это аналог схемы в реляционных БД.
В одной Keyspace может быть несколько таблиц.
Таблица очень похожа на реляционную таблицу. 
Столбцы - это атрибуты, а строки - конкретные записи.

### Partition key
Один (или несколько вместе) из столбцов должен являться partition key - ключом партиции.
Выбор ключа партиции очень важен, т.к. именно по нему данные в таблице дробятся на партиции и реплицируются.
Т.е. данные по различным значениям ключа должны быть распределены равномерно, иначе какие-то ноды будут нагружены очень сильно, а другие будут прохлаждаться.


## Архитектура:
- Кластерный (core)
  - gossip
  - cluster state
  - replication
  - partiioner
- Локальный (middle) - все что касается работы на одной машине
  - memtable
  - sstable
  - indexes
  - compaction 
  - bloom filter
  - commit log
  - compaction
- Верхний (top)
  - tombstones
  - monitoring
  - admin tools
  - hinted handoff 

## Топология сети
Snitch - способ, которым узел определяет топологию сети (датацентр-стойка-узел).

В каждом ЦОД - свое кольцо.

Виды snitch:
- `GossipingPropertyFileSnitch` - на каждой ноде свой файлик, в котором прописано в каком ЦОД и в какой стойке находится сервак.
- ...


## Чинилки
### Read Repair
Если при чтении была замечена неконсистентность, то координатор высылает нодам, в которых были замечены устаревшие данные, более свежие данные.

### Anti-Entropy Node Repair
Для того чтобы починить данные, которые редко читаются (и потому не могут быть автоматически вылечены read repair'ом), можно использовать `nodetool`.
`nodetool` - это консольная утилита, которая поставляется вместе с кассандрой.
```sh
nodetool repair <keyspace> [table] <opts>
```

Желательно вешать крон на эту операцию и вызывать ее раз в некоторое время.

Эта операция repair'а строит Merkle Tree на каждой ноде, хранящей таблицу, которую мы чиним. 
Деревья сравниваются между собой и выявляются неконсистентные данные.


---
## Nodetool
- `ring` - посмотреть на кольцо и на принадлежность токенов, его образующего
- `status` - посмотреть на состояние кластера
- `drain` - останавливает запись и сливает все данные из memtable в sstable. Используется для подготовки ноды к отключению
- `flush` - сливает все данные из memtable в sstable
- `describecluster` - верхнеуровневая информация о кластере
- `compactionhistory` - история операций компакции


---
## Проектирование инфраструктуры
Пусть планируемый объем данных - 1 ТБ.
Тогда необходимо иметь 1 ТБ * `kr`, где `kr` - это коэффициент репликации, чтобы обеспечить необходимую надежность.
Еще необходимо заложить избыточность в 2 раза, чтобы данные в sstable могли быть закомпакчены.

Т.е. для хранения 1 ТБ данных нужно предусмотреть 6 ТБ дисков - 3 по 2 ТБ.


---
## Миграции
Основные фреймворки для миграций схем баз данных поддерживают работу с Cassandra.
Но существуют и утилиты, заточенные только под Cassandra:
- [Cassandra Migration](https://github.com/Contrast-Security-OSS/cassandra-migration)
- [Cassandra Migrate](https://github.com/Cobliteam/cassandra-migrate) - питоновский CLI
- [cqlmigrate](https://github.com/sky-uk/cqlmigrate) - библиотечка, подключаемая к проекту в виде gradle или maven плагина
- [Cassper](https://github.com/dataoperandz/cassper)

### Liquibase
[Liquibase](liquibase.md) позволяет описывать миграции в `.cql`-файлах с changelog'ами.
Разбиение файла на changelog'ов выполняется с помощью комментариев подобно тому, как это делается для `.sql`-файлов.

Для функционирования Liquibase понадобится JDBC драйвер Cassandra и расширение Liquibase-Cassandra.

---
## К изучению

- [ ] [Официальная документация](https://cassandra.apache.org/doc/latest/cassandra/architecture/index.html)
- [X] [Воркшоп от dev advocate Apache Cassandra](https://www.youtube.com/watch?v=Ae4GABykRoM&t=25s&ab_channel=DataStaxDevelopers)
- [X] [Миграции Liquibase](https://docs.liquibase.com/install/tutorials/cassandra.html)